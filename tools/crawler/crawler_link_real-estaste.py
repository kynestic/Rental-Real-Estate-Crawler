import requests
import re
import pandas as pd
import os
import time

# Function to fetch and process the page with retry
def fetch_page(page_num, max_retries=8, delay=3):
    url = f"https://www.realtor.com/apartments/90011/pg-{page_num}"
    payload = { 
        'api_key': '787aca77cfbd06ef9f45c8ec19626a31', 
        'url': url, 
        'wait_for_selector': 'ul li'
    }

    retries = 0
    while retries < max_retries:
        try:
            r = requests.get('https://api.scraperapi.com/', params=payload)
            if r.status_code == 200:
                # Ghi ná»™i dung vÃ o file tmp.html
                with open('tmp.html', 'w', encoding='utf-8') as file:
                    file.write(r.text)
                print(f"âœ… ÄÃ£ ghi HTML cá»§a trang {page_num} vÃ o tmp.html.")
                return True
            elif r.status_code == 500:
                retries += 1
                print(f"âš ï¸ Lá»—i 500 táº¡i trang {page_num}. Thá»­ láº¡i ({retries}/{max_retries}) sau {delay}s...")
                time.sleep(delay)
            else:
                print(f"âŒ Request tháº¥t báº¡i cho trang {page_num}, mÃ£ lá»—i: {r.status_code}")
                return False
        except Exception as e:
            print(f"â€¼ï¸ Lá»—i khi request trang {page_num}: {e}")
            return False

    print(f"âŒ QuÃ¡ sá»‘ láº§n thá»­ láº¡i cho trang {page_num}. Bá» qua.")
    return False

# Function to extract URLs and ensure they start with 'https://www.zillow.com/'
def extract_urls():
    with open('tmp.html', 'r', encoding='utf-8') as file:
        content = file.read()

    # Sá»­ dá»¥ng regex Ä‘á»ƒ tÃ¬m cÃ¡c URL vá»›i "detailUrl"
    urls = re.findall(r'"detailUrl":"(.*?)"', content)

    if urls:
        url_list = []
        print("ðŸ”— CÃ¡c URL tÃ¬m Ä‘Æ°á»£c:")
        for url in urls:
            if not url.startswith("https://www.zillow.com/"):
                url = "https://www.zillow.com" + url
            url_list.append(url)
            print(url)

        # Ghi vÃ o CSV
        df = pd.DataFrame(url_list, columns=["URL"])
        if os.path.exists('urls.csv'):
            df.to_csv("urls.csv", mode='a', header=False, index=False)
        else:
            df.to_csv("urls.csv", mode='w', header=True, index=False)
        
        print("ðŸ“¥ ÄÃ£ thÃªm cÃ¡c URL vÃ o file urls.csv.")
    else:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y URL nÃ o.")

# Duyá»‡t qua cÃ¡c trang
for page_num in range(1, 21):
    if fetch_page(page_num):
        extract_urls()
